\documentclass{beamer}

\usepackage{graphicx}
\usepackage{hyperref}

\begin{document}
\begin{frame}
  \frametitle{Goals}

  \begin{itemize}
  \item Understand capabilities of commodity computing hardware.
    \begin{itemize}
    \item Test data throughput, check for show stoppers.
    \item Determine required hardware specs.
    \item Guide design/scaling/fragmentation.
    \end{itemize}
  \item Develop toy but realistic prototype
    \begin{itemize}
    \item Understand multi-threading design issues.
    \item Full size ring buffer.
    \item Prototype algorithms on realistic simulated data.
    \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Test Host Computers}
  
  CPU and RAM specs:
  \begin{description}
  \item[hal] i5-2520M 2.5GHz (4 HT cores) \\ 16GB DDR3 1600 (12 GB/s max)
  \item[haiku] 17-4770k 3.5G Hz (8 HT cores) \\ 16 GB DDR3 1333 (10.7 GB/s max)
  \item[hierocles] Xeon E5-2630 2.2GHz (40 HT cores) \\ 64 GB DDR4 ECC 2400 (at 2133, 17 GB/s max)
  \item[hothstor] i7-7700K 4.2 GHz (8 HT cores) \\ 32 GB DDR4 2400 (19 GB/s)
  \end{description}

\end{frame}

\begin{frame}
  \frametitle{Multi-threaded Ring Buffer Design}

  Software design findings:
  \begin{itemize}
  \item One thread writes, another reads, requires thread safety.
  \item First try with \texttt{std::mutex} protection on \texttt{push()}/\texttt{pop()}.
  \item ``Lockless'' with \texttt{std::atomic} \textbf{gain $13\times$ throughput}.
  \item The \texttt{operator\%} is expensive, use $2^n$ buffer length and replace modulo with bit-mask, \textbf{gain $2\times$ throughput}.
  \item 10-20\% speedup with \texttt{size\_t} $\to$ 32bit indices, but:
    \begin{itemize}
    \item[$\times$] 32bits of 0.5 $\mu$s ticks overflows in 35 minutes.
    \item[\checkmark] 64bits gives us almost 300 millennia.
    \end{itemize}
  \item Per slot allocation (\texttt{new[]}/\texttt{delete[]}) is slow.
    \begin{itemize}
    \item Pre-allocate large $N_{ticks} \times N_{chan}$ block of \texttt{short}s.
    \item \texttt{push()} returns to client a pointer to the array to use.
    \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}[fragile,fragile]
  \frametitle{Test Code}
  \begin{center}
    \url{https://github.com/brettviren/thread-tests}
  \end{center}
Install \href{http://www.fabfile.org/}{Fabric} and download the provided \href{https://raw.githubusercontent.com/brettviren/thread-tests/master/fabfile.py}{fabfile.py}.
\begin{verbatim}
$ fab -P -H host1,host2,host3 doitall
\end{verbatim}

\begin{enumerate}
\item SSH's to each host
\item \texttt{git clone} the source to a tempdir
\item Builds it with included Waf \texttt{wscript}.
  \begin{itemize}
  \item Needs BOOST, C++ compilers.
  \end{itemize}
\item Runs test programs providing timing.
\item Prints summary (see next slide).
\item Cleans up temporary working directory.
\item Creates local log file for each host with results.
\end{enumerate}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Example Raw Results}
  
Raw results saved local log file at end of each hosts' run:
\tiny
\begin{verbatim}
$ cat haiku.out
host:haiku
with mutex
 25.632795s wall, 31.880000s user + 19.350000s system = 51.230000s CPU (199.9%)
with atomic + bitmask
 1.814277s wall, 3.630000s user + 0.010000s system = 3.640000s CPU (200.6%)
with atomic + modulo
 0.986726s wall, 1.950000s user + 0.000000s system = 1.950000s CPU (197.6%)
bufsize: 2^18 width=4096 nelements=100000000
 92.448756s wall, 180.330000s user + 4.260000s system = 184.590000s CPU (199.7%)
model name	: Intel(R) Core(TM) i7-4770K CPU @ 3.50GHz
ncores: 8
total        used        free      shared  buff/cache   available
Mem:             15           1           4           0           9          13
Swap:            23           0          23
\end{verbatim}
\end{frame}

\begin{frame}
  \frametitle{Test Programs}
  \texttt{test\_*}:
  \begin{description}
  \item[\texttt{modulo}] runs $10^8$ \textbf{integers} into ring of
    size $2^{20}$, runs three variants of ring buffer implementation.
    \begin{itemize}
    \item \texttt{std::mutex}-based thread protection
    \item \texttt{std::atomic} + \texttt{operator\%} ring indexing
    \item \texttt{std::atomic} + bitmask ring indexing
    \end{itemize}
  \item[\texttt{arene}] runs $10^8$ \textbf{tick vectors} through ring
    of size $4096 \times 2^{18}$ ($N_{chan} \times N_{tick}$) with a
    \texttt{memcpy()} on every \texttt{push()} and \texttt{pop()}.
    \begin{itemize}
    \item Corresponds to 819 GB throughput.
    \item Each tick is two \texttt{memcpy()} so RAM TP is twice.
    \end{itemize}
  \end{description}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Summary of Results of Ring Buffer Tests}
  \begin{center}
    \begin{tabular}[h]{|r||r|r|r||r|r|r|}
      \hline
      host&mutex&atomic&atomic&arene&TP     & Max \\
          &     &modulo&bitmask&    & (GB/s)& (GB/s)\\
      \hline
      hierocles & 31s & 2.8s & 2.3s & 227s & 3.6 & 17.0 \\
      hal       & 35s & 3.5s & 1.5s & 206s & 4.0 & 12.0 \\
      haiku     & 26s & 1.8s & 1.0s &  92s & 8.9 & 10.7 \\
      hothstor  & 25s & 1.6s & 0.9s &  48s & 16.9& 19.0 \\
      \hline
    \end{tabular}
  \end{center}
  \begin{itemize}
  \item The throughput (TP) here is the naive count and does not include
    that each tick vector has two \texttt{memcpy()} calls.
    \begin{itemize}\scriptsize
    \item[$\to$] A little confused why \texttt{hothstor} result is so
      close to max.
    \end{itemize}
  \item \texttt{hierocles} doesn't achieve full 200\% CPU on
    \texttt{mutex} test.  High core count (40) but slow.  Similar
    memory to \texttt{hothstor}.
    \begin{itemize}\scriptsize
    \item[$\to$] Both memory and CPU speeds matter!
    \end{itemize}
  \end{itemize}
  Conclusion: DDR4 and 4GHz CPU will allow $>$10GB/s ring buffer.

  \footnotesize
  $\to$ But, still many other bottlenecks to worry about!
\end{frame}

\begin{frame}[fragile]
  \frametitle{Unix Domain Socket Test}
  \begin{itemize}
  \item I have no FELIX nor 100 Gbps NICs hardware to test with.
  \item Use Unix Domain Sockets as a test, no idea if it is relevant.
  \item Rely on \texttt{socat} to do the test because lazy.  Copy \texttt{/dev/zero} through a FIFO socket.
    \begin{enumerate}
    \item 1MB $\times 10240$ 
    \item 8192 $\times 1024000$ 
    \end{enumerate}
  \end{itemize}
  
\tiny
\begin{verbatim}
socat -u -b32768 UNIX-LISTEN:/tmp/unix.sock /dev/null &
socat -u -b32768 "SYSTEM:dd if=/dev/zero bs=1M count=10240" UNIX:/tmp/unix.sock
socat -u -b32768 UNIX-LISTEN:/tmp/unix.sock /dev/null &
socat -u -b32768 "SYSTEM:dd if=/dev/zero bs=8192 count=1024000" UNIX:/tmp/unix.sock
\end{verbatim}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Summary of Unix Domain Socket Tests}
  
  \begin{center}
    \begin{tabular}[h]{|r|r|r|}
      \hline
      host & 11GB/1MB & 8.4GB/8K \\
           & (GB/s) & (GB/s) \\
      \hline
      hal & 2.2 & 2.1 \\
      hierocles & 3.0 & 2.8 \\
      haiku & 4.3 & 4.3  \\
      hothstor & 4.3 & 5.7 \\
      \hline
    \end{tabular}
  \end{center}
  Conclusion: While \texttt{socat} copying via Unix Domain Socket is
  not directly applicable, required 10 GB/s is not achieved.
  Important to test with actual eg, hardware (FELIX and/or 100 Gbps NIC).
\end{frame}

\end{document}
