% TEXINPUTS=.:$HOME/git/bvtex: latexmk  -pdf <main>.tex

\documentclass[xcolor=dvipsnames]{beamer}

\input{defaults}
\input{beamer/preamble}
\setbeamertemplate{navigation symbols}{}

\definecolor{bvtitlecolor}{rgb}{0.98, 0.92, 0.84}
\definecolor{bvoutline}{rgb}{0.1, 0.1, 0.1}

\renewcommand{\bvtitleauthor}{Brett Viren}
\renewcommand{\bvtit}{}
\renewcommand{\bvtitle}{\LARGE Some Tests for Ring Buffer \\ using Commodity CPU+RAM}
\renewcommand{\bvevent}{}
\renewcommand{\bvbeamerbackground}{}

\begin{document}
\input{beamer/title.tex}
%\input{beamer/toc.tex}

\begin{frame}[fragile]
  \frametitle{Ring Buffer using Commodity CPU+RAM}

  \begin{center}
    One thread pushing, another thread popping.

    \includegraphics[height=2cm]{ring.pdf}

    How to code it? Can it keep up?

    How does the CPU/RAM hardware matter?

    \vfill

    And, one partial extra: How fast can we get data from OS?
    
    \includegraphics[height=2cm]{push.pdf}
  \end{center}
  


\end{frame}


\begin{frame}
  \frametitle{Multi-threaded Ring Buffer Software Design}

  Learn/leverage others: \texttt{std::thread}, BOOST, TBB.
  \begin{itemize}
  \item[+] Try \texttt{std::mutex} protection on \texttt{push()}/\texttt{pop()}.
  \item[+] Then, ``lockless'' \texttt{std::atomic} for tail/head indices.
    \begin{itemize}\footnotesize
    \item[--] requires monotonic increasing indices.
    \item[\checkmark] \textbf{Gain $\sim 10\times$ throughput}.
    \end{itemize}
  \item[+] Then, use buffer size $2^n$, replace \textbf{modulo} with
    \textbf{bitmask}.
    \begin{itemize}\footnotesize
    \item[--] requires maybe bigger buffer than nominal.
    \item[\checkmark] \textbf{Gain $\sim 2\times$ throughput}.
    \end{itemize}
  \item[+] 10-20\% more speedup with \texttt{size\_t} $\to$ 32bit indices, but:
    \begin{itemize}\footnotesize
    \item[$\times$] 32bits of 0.5 $\mu$s ticks overflows in 35 minutes.
    \item[\checkmark] 64bits gives us almost 300 millennia.
    \end{itemize}
  \item[+] Finally, many (\texttt{new[]}/\texttt{delete[]}) is slow and fragmentary.
    \begin{itemize}\footnotesize
    \item[$\to$] Pre-allocate large $N_{ticks} \times N_{chan}$ block
      of \texttt{short}s.
    \item[$\to$] \texttt{push()} gives client a pointer to the
      array memory to use.
    \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}[fragile,fragile]
  \frametitle{Test Code}
  Development at:
  \begin{center}
    \url{https://github.com/brettviren/thread-tests}
  \end{center}
Reproduce using \href{http://www.fabfile.org/}{Fabric} and the provided \href{https://raw.githubusercontent.com/brettviren/thread-tests/master/fabfile.py}{fabfile.py}.
\begin{verbatim}
$ fab -P -H host1,host2,host3 doitall
\end{verbatim}
It will:
\begin{enumerate}\footnotesize
\item SSH's to each host and \texttt{git clone} the source to a tempdir.
\item Builds test code, needs BOOST, C++ compilers, etc.
\item Runs test programs, checking timing.
\item Cleans up temporary working directory.
\item Saves each host results to local log file.
\end{enumerate}
\end{frame}



\begin{frame}
  \frametitle{Test Host Computers}
  
  Hosts' CPU and RAM specs:
  \begin{description}
  \item[hal] i5-2520M 2.5GHz (4 HT cores) \\ 16GB DDR3 1600 (12 GB/s max)
  \item[haiku] 17-4770k 3.5G Hz (8 HT cores) \\ 16 GB DDR3 1333 (10.7 GB/s max)
  \item[hierocles] Xeon E5-2630 2.2GHz (40 HT cores) \\ 64 GB DDR4 ECC 2400 (at 2133, 17 GB/s max)
  \item[hothstor] i7-7700K 4.2 GHz (8 HT cores) \\ 32 GB DDR4 2400 (19 GB/s max)
  \end{description}
  \begin{center}\footnotesize
    (max DDR speeds are from wikipedia)    
  \end{center}
\end{frame}

\begin{frame}
  \frametitle{Ring Buffer Test Programs}
  \texttt{test\_*}:
  \begin{description}
  \item[\texttt{modulo}] sends $10^8$ \textbf{integers} into ring of
    size $2^{20}$.\\
    Three variants of ring buffer implementation:    
    \begin{enumerate}
    \item \texttt{std::mutex}-based thread protection
    \item \texttt{std::atomic} + \texttt{operator\%} ring indexing
    \item \texttt{std::atomic} + bitmask ring indexing
    \end{enumerate}
  \item[\texttt{arene}] sends $10^8$ \textbf{tick vectors} into ring of size $ 2^{18}$
    \begin{itemize}
    \item Use \texttt{std::atomic} + bitmask design.
    \item Tick vectors: 4096 ``channels'' with 2B samples.
    \item Test corresponds to 819 GB apparent transfer.
    \item But, two \texttt{memcpy()}'s: one for each \texttt{push()} and
      \texttt{pop()} so really RAM transfer is $2\times$ more.
    \end{itemize}
  \end{description}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Summary of Results of Ring Buffer Tests}
  \begin{center}
    \begin{tabular}[h]{|r||r|r|r||r|r|r|}
      \hline
      host&mutex&atomic&atomic&arene&TP     & Max \\
          &     &modulo&bitmask&    & (GB/s)& (GB/s)\\
      \hline
      hierocles & 31s & 2.8s & 2.3s & 227s & 3.6 & 17.0 \\
      hal       & 35s & 3.5s & 1.5s & 206s & 4.0 & 12.0 \\
      haiku     & 26s & 1.8s & 1.0s &  92s & 8.9 & 10.7 \\
      hothstor  & 25s & 1.6s & 0.9s &  48s & 16.9& 19.0 \\
      \hline
    \end{tabular}
  \end{center}
  \begin{itemize}\footnotesize
  \item The throughput (TP) here is the naive count and does not include
    the double \texttt{memcpy()} calls.
    \begin{itemize}\scriptsize
    \item[$\therefore$] I'm a little confused why \texttt{hothstor} result is so
      close to max.
    \end{itemize}
  \item \texttt{hierocles} has fast RAM but slow CPU.
    \texttt{haiku}/\texttt{hothstor} have similar CPU, but much
    different RAM.
  \item[$\to$] Both CPU and RAM matter!
  \end{itemize}
  Conclusion: DDR4 and 4GHz CPU will allow $>$10GB/s ring buffer.

\end{frame}

\begin{frame}[fragile]
  \frametitle{Unix Domain Socket Test}
  How fast can OS make data available to push to ring?
  \begin{center}
    \includegraphics[height=2cm]{socat.pdf}    
  \end{center}

  \begin{itemize}
  \item I have no FELIX nor 100 Gbps NICs hardware to test with.
  \item Use Unix Domain Sockets as easy test
    \begin{itemize}\footnotesize
    \item  \textbf{This may be a totally irrelevant test} (but it's easy to make)
    \item Rely on \texttt{socat} to do the lifting.
    \end{itemize}
  \item Approximate possible $\sim$ 1ms chunk sizes:
    \begin{description}
    \item[wib block] 1 MB $\times 10240$ = 11 GB 
    \item[tick vector] 8192 B $\times 1024000$ = 8.4 GB
    \end{description}
  \end{itemize}
  
\end{frame}

\begin{frame}[fragile]
  \frametitle{Summary of Unix Domain Socket Tests}
  
  \begin{center}
    \begin{tabular}[h]{|r|r|r|}
      \hline
      host & wib block & tick vector \\
           & (GB/s) & (GB/s) \\
      \hline
      hal & 2.2 & 2.1 \\
      hierocles & 3.0 & 2.8 \\
      haiku & 4.3 & 4.3  \\
      hothstor & 4.3 & 5.7 \\
      \hline
    \end{tabular}
  \end{center}
  Conclusion: While \texttt{socat} copying via Unix Domain Socket may
  not be applicable, the required 10 GB/s is not achieved.  Important to
  test with actual eg, hardware (FELIX and/or 100 Gbps NIC).
\end{frame}

\begin{frame}
  \frametitle{Summary and Further}
  \begin{itemize}
  \item[+] A ring buffer with better than 10 GB/s throughput is feasible
    with today's commodity CPU+RAM.
    \begin{itemize}\footnotesize
    \item[!] Careful threaded software design needed to achieve performance.
    \end{itemize}
  \item[?] Simple Unix Domain Socket (FIFO) test achieves only $\sim$50\%
    required minimum throughput,  but it is not a conclusive test.
    \begin{itemize}\footnotesize
    \item[!] FELIX will be tested as part of protoDUNE.  Suggest perform
      some sustained (many seconds) full-rate 10 GB/s transfers.
    \item[\checkmark] Found \href{http://iopscience.iop.org/article/10.1088/1742-6596/664/8/082050}{CHEP2015 proceedings} which demonstrate FELIX's FPGA can DMA at 10 GByte/sec.
    \end{itemize}
  \item[$\to$] Will start following up on pointers from FELIX experts on more
    sophisticated software designs for ring buffers.
  \end{itemize}
\end{frame}

\end{document}
